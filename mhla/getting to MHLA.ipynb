{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1899df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69823ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    # hyperparameters\n",
    "    batch_size : int # how many independent sequences will we process in parallel?\n",
    "    block_size : int  # what is the maximum context length for predictions?\n",
    "    vocab_size : int # OPTIM 4 (along with grad clipping) brought dt from 95 to 90\n",
    "\n",
    "    max_iters : int\n",
    "    eval_interval : int\n",
    "    learning_rate : float\n",
    "    warmup_steps : int\n",
    "    max_decay_steps : int\n",
    "\n",
    "    device : str\n",
    "    eval_iters : int\n",
    "    compile : bool #= False if os.name != 'posix' else True\n",
    "    save_model : bool\n",
    "\n",
    "    kv_latent_dim : int\n",
    "    q_latent_dim : int\n",
    "    n_embd : int\n",
    "    n_head : int\n",
    "    n_layer : int\n",
    "    n_kv_heads : int # Set to 6 for MHA, 1 for MQA, or another divisor of n_head for GQA\n",
    "    dropout : float\n",
    "    total_batch_size : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3093bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHLA(nn.Module):\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Projection layers\n",
    "        self.W_dq  = nn.Linear(config.n_embd,        config.q_latent_dim,  bias=False)  # Query down projection\n",
    "        self.W_uq  = nn.Linear(config.q_latent_dim,  config.n_embd,        bias=False)  # Query up projection\n",
    "        self.W_dkv = nn.Linear(config.n_embd,        config.kv_latent_dim, bias=False)  # Compress into latent KV space\n",
    "        self.W_uk  = nn.Linear(config.kv_latent_dim, config.n_embd,        bias=False)  # Decompress K\n",
    "        self.W_uv  = nn.Linear(config.kv_latent_dim, config.n_embd,        bias=False)  # Decompress V\n",
    "        self.W_o   = nn.Linear(config.n_embd,        config.n_embd,        bias=False)  # Final output projection\n",
    "\n",
    "        self.ln = nn.LayerNorm(config.kv_latent_dim)\n",
    "        self.register_buffer('absorbed_k', None)  # Holds W_q @ W_uk\n",
    "\n",
    "    def forward(self, x, kv_cache=None, past_length=0):\n",
    "        B, S, D = x.size()\n",
    "\n",
    "        # Compute absorbed_k once: W_q @ W_uk, shape: (D, latent_dim)\n",
    "        if self.absorbed_k is None:\n",
    "            absorbed = torch.matmul(self.W_q.weight, self.W_uk.weight)\n",
    "            self.absorbed_k = absorbed.view(self.n_heads, self.dh, -1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
