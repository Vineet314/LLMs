{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from typing import Literal, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "@dataclass\n",
    "class LLMconfig:\n",
    "    pos_emb: str | Literal['learn', 'sin', 'rope']\n",
    "    # attn\n",
    "    attn: str | Literal['mha', 'mqa', 'gqa', 'mla']\n",
    "    n_head: int\n",
    "    # ffn\n",
    "    moe: bool\n",
    "    up_dim: int\n",
    "    non_linearity: str | Literal['elu', 'lrelu', 'relu', 'gelu', 'swish', 'mish', 'silu','selu', 'celu', 'tanh', 'swiglu', 'sigmoid']\n",
    "\n",
    "    # token params\n",
    "    vocab_size : int\n",
    "    block_size : int\n",
    "    n_embd : int\n",
    "\n",
    "    # model params\n",
    "    dropout : float\n",
    "    n_layer : int\n",
    "    norm : str | Literal['layer','rms']\n",
    "\n",
    "    act_recomp : bool  # more of a training param, but the best way to integrate that is to just add it here\n",
    "\n",
    "    # Optional fields with default as None\n",
    "    # attn\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    q_latent_dim:  Optional[int] = None\n",
    "    kv_latent_dim: Optional[int] = None\n",
    "    rope_head_dim: Optional[int] = None\n",
    "    # ffn\n",
    "    n_exp:    Optional[int] = None\n",
    "    n_shared: Optional[int] = None\n",
    "    n_act:    Optional[int] = None\n",
    "    coeff:    Optional[float] = None\n",
    "    aux_free: Optional[bool] = None\n",
    "    alpha:    Optional[float] = None\n",
    "    gamma:    Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class Trainconfig:\n",
    "    dataset : str | Literal['shakespeare', 'tinystories', 'fineweb', 'wikitext']\n",
    "    total_batch_size : int\n",
    "    batch_size : int\n",
    "    max_iters : int\n",
    "    eval : bool\n",
    "    eval_interval : int\n",
    "    eval_iters : int\n",
    "    learning_rate : float\n",
    "    warmup_steps : int\n",
    "    grad_clip : int\n",
    "    compile : bool #= False if os.name != 'posix' else True\n",
    "    save_model : bool\n",
    "    ckpt_interval : int\n",
    "    file_name : str\n",
    "    act_recomp : bool\n",
    "    wandb_log : bool\n",
    "    wandb_project : str\n",
    "    wandb_run_name : str\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T, file_path, device):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.file_path = file_path\n",
    "        self.device = device\n",
    "        self.device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "        # Keep the memory-mapped file open persistently\n",
    "        self.tokens = np.memmap(self.file_path, dtype=np.uint16, mode='r')\n",
    "        self.N = len(self.tokens)\n",
    "        if self.B * self.T + 1 > self.N:\n",
    "            raise ValueError(f\"Batch size {B} and block size {T} are too large for dataset of length {self.N}\")\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Returns (x, y) where:\n",
    "        - x is (B, T) input tokens\n",
    "        - y is (B, T) target tokens (shifted by one)\n",
    "        \"\"\"\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # Sample B random starting positions independently\n",
    "        start_indices = torch.randint(0, self.N - T - 1, (B,))\n",
    "\n",
    "        # Gather sequences\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        for start in start_indices:\n",
    "            seq = self.tokens[start : start + T + 1].astype(np.int64)\n",
    "            x_list.append(seq[:-1])\n",
    "            y_list.append(seq[1:])\n",
    "\n",
    "        # Stack into tensors\n",
    "        x = torch.tensor(np.stack(x_list), dtype=torch.long)\n",
    "        y = torch.tensor(np.stack(y_list), dtype=torch.long)\n",
    "\n",
    "        # Move to device (with pinned memory if CUDA)\n",
    "        if self.device_type == 'cuda':\n",
    "            x = x.pin_memory().to(self.device, non_blocking=True)\n",
    "            y = y.pin_memory().to(self.device, non_blocking=True)\n",
    "        else:\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "        return x, y\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    ''' Implements the given type of layer normalization (either Vanilla Layer Norm or RMS-Layer Norm)'''\n",
    "    def __init__(self, config:LLMconfig, dim:int):\n",
    "        super().__init__()\n",
    "        if config.norm == 'layer': \n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "            self.bias  = self.norm.bias\n",
    "        elif config.norm == 'rms':\n",
    "            self.norm = nn.RMSNorm(dim)\n",
    "        self.weight = self.norm.weight\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "    \n",
    "class GQA(nn.Module):\n",
    "    \"\"\" Grouped-Query Attention with or without RoPE \"\"\"\n",
    "\n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        if config.attn == 'mha' : config.n_kv_heads = config.n_head\n",
    "        elif config.attn == 'mqa' : config.n_kv_heads = 1\n",
    "        else : assert config.n_head % config.n_kv_heads == 0, \"n_head must be divisible by n_kv_heads\"\n",
    "        \n",
    "        assert config.n_embd % config.n_head == 0, \"n_embd must be divisible by n_head\"\n",
    "        self.config = config\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "\n",
    "        # k,q,v in a btach\n",
    "        self.c_attn = nn.Linear(config.n_embd, config.n_embd + 2 * config.n_kv_heads * self.head_size)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout  = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, freqs_cis:torch.Tensor|None, kv_cache=None, VAL_RUN=False):\n",
    "        B, T, C = x.size()\n",
    "        nh, nkvh, hs = self.config.n_head , self.config.n_kv_heads, self.head_size\n",
    "\n",
    "        q_proj_size = C # n_embd\n",
    "        kv_proj_size = nkvh * hs\n",
    "        q, k, v = self.c_attn(x).split([q_proj_size, kv_proj_size, kv_proj_size], dim=2)\n",
    "        q:torch.Tensor = q.view(B, T, nh, hs)                   # (B, T, nh, hs)\n",
    "        k:torch.Tensor = k.view(B, T, nkvh, hs)                 # (B, T, n_kvh, hs)\n",
    "        v:torch.Tensor = v.view(B, T, nkvh, hs).transpose(1, 2) # (B, n_kvh, T, hs)\n",
    "\n",
    "        if self.config.pos_emb == 'rope':\n",
    "        # Apply RoPE\n",
    "            q = apply_rotary_emb(q, freqs_cis)\n",
    "            k = apply_rotary_emb(k, freqs_cis)\n",
    "\n",
    "        q,k = q.transpose(1, 2), k.transpose(1, 2) # (B, nh, T, hs) # (B, n_kvh, T, hs)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            past_k, past_v = kv_cache\n",
    "            k = torch.cat((past_k, k), dim=-2)\n",
    "            v = torch.cat((past_v, v), dim=-2)\n",
    "\n",
    "        updated_kv_cache = (k, v)\n",
    "\n",
    "        if nkvh != nh:\n",
    "            num_repeats = nh // nkvh\n",
    "            k = k.repeat_interleave(num_repeats, dim=1)\n",
    "            v = v.repeat_interleave(num_repeats, dim=1)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.config.dropout if self.training else 0, is_causal=True)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "        return y, updated_kv_cache\n",
    "\n",
    "class NaiveMHLA(nn.Module):\n",
    "    \"\"\" A fully parallel implementation of the MHLA algorithm without the RoPE. No for loops.\"\"\"\n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, \"num of heads must be a divisor of n_embd\"\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        self.config = config\n",
    "\n",
    "        self.W_dq  = nn.Linear(config.n_embd,        config.q_latent_dim,  bias=False)\n",
    "        self.W_uq  = nn.Linear(config.q_latent_dim,  config.n_embd,        bias=False)\n",
    "        self.W_dkv = nn.Linear(config.n_embd,        config.kv_latent_dim, bias=False)\n",
    "        self.W_uk  = nn.Linear(config.kv_latent_dim, config.n_embd,        bias=False)\n",
    "        self.W_uv  = nn.Linear(config.kv_latent_dim, config.n_embd,        bias=False)\n",
    "        self.W_o   = nn.Linear(config.n_embd,        config.n_embd,        bias=False)\n",
    "        \n",
    "        # self.ln  = LayerNorm(config, config.kv_latent_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.register_buffer('_k_absorbed_inference', None, persistent=True)\n",
    "        self.register_buffer('_v_absorbed_inference', None, persistent=True)\n",
    "\n",
    "    def _precompute_absorbed_matrices(self):\n",
    "        \"\"\"Precomputes k_absorbed and v_absorbed for efficient inference.\"\"\"\n",
    "        # Just to be safe\n",
    "        if (self._k_absorbed_inference is not None) and (self._v_absorbed_inference is not None):\n",
    "            return \n",
    "        \n",
    "        nh , n_kvl, hs = self.config.n_head, self.config.kv_latent_dim, self.head_size\n",
    "        with torch.no_grad():\n",
    "            self._k_absorbed_inference = (self.W_dq.weight.T @ self.W_uq.weight.T  @ self.W_uk.weight).view(nh, hs, n_kvl).unsqueeze(0)\n",
    "            self._v_absorbed_inference = (self.W_uv.weight.T @ self.W_o.weight.T).view(n_kvl, nh, hs).transpose(0,1).unsqueeze(0)    \n",
    "\n",
    "    def forward(self, x:torch.Tensor, freqs_cis:torch.Tensor|None, kv_cache=None, VAL_RUN=False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        B, T, C = x.size()\n",
    "        nh, n_kvl, hs = self.config.n_head, self.config.kv_latent_dim, self.config.n_embd//self.config.n_head\n",
    "\n",
    "        # k_eff and v_eff based on training or inference\n",
    "        if self.training or VAL_RUN: # HIDDEN IN THE DEPTHS : THIS BUG TOOK ~16 HRS TO DEBUG\n",
    "            k_eff = (self.W_dq.weight.T @ self.W_uq.weight.T  @ self.W_uk.weight).view(nh, hs, n_kvl).unsqueeze(0)\n",
    "            v_eff = (self.W_uv.weight.T @ self.W_o.weight.T).view(n_kvl, nh, hs).transpose(0,1).unsqueeze(0)\n",
    "        else:\n",
    "            if (self._k_absorbed_inference is None) or (self._v_absorbed_inference is None):\n",
    "                self._precompute_absorbed_matrices()\n",
    "            k_eff = self._k_absorbed_inference\n",
    "            v_eff = self._v_absorbed_inference\n",
    "        \n",
    "        new_c_kv = self.W_dkv(x) # down projection : (B,T,C) -> (B,T,n_kvl)\n",
    "\n",
    "        if kv_cache is None:\n",
    "            c_kv = new_c_kv # (B,T,n_kvl) ; initiate cache\n",
    "        else:\n",
    "            c_kv = torch.cat([kv_cache, new_c_kv], dim=1) # append cache\n",
    "        \n",
    "        updated_kv_cache = c_kv\n",
    "\n",
    "        T_full = c_kv.size(1) # Current total sequence length (including cache)\n",
    "\n",
    "        q:torch.Tensor = self.W_uq(self.W_dq(x)) # query projection : (B,T,C) -> (B,T,n_ql) -> (B,T,C)\n",
    "        q = q.view(B, T, nh, hs).transpose(1, 2) # (B,T,C) -> (B,T,nh,hs) -> (B, nh, T, hs)\n",
    "\n",
    "        attn:torch.Tensor = (q @ k_eff @ c_kv.transpose(1,2).unsqueeze(1)) / math.sqrt(hs)\n",
    "\n",
    "        # query_indices = torch.arange(T, device=x.device).unsqueeze(1) + (T_full - T)\n",
    "        # key_indices   = torch.arange(T_full, device=x.device).unsqueeze(0)\n",
    "        # mask = (query_indices >= key_indices).unsqueeze(0).unsqueeze(0) # (1,1,T,T_full)\n",
    "        # attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        mask = torch.triu(torch.ones(T, T_full, device=x.device, dtype=torch.bool), diagonal=T_full - T + 1)\n",
    "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        attn = self.dropout(F.softmax(attn, dim=-1)) # (B, nh, T, T_full)\n",
    "\n",
    "        # final output : attn @ C_kv @ v_abs \n",
    "        # (B, nh, T, T) * (B, 1, T, n_kvl) * (1, nh, n_kvl, hs) = (B, nh, T, hs)\n",
    "        y:torch.Tensor = attn @ c_kv.unsqueeze(1) @ v_eff #(B, nh, T, hs)\n",
    "        y = self.dropout(y.transpose(1,2).contiguous().view(B,T,C))\n",
    "\n",
    "        return y, updated_kv_cache\n",
    "\n",
    "class FullMHLA(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully parallel implementation of Multi-Head Latent Attention (MLA)\n",
    "    with Decoupled Rotary Position Embeddings (RoPE), as described in DeepSeek-V2.\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0, \"num of heads must be a divisor of n_embd\"\n",
    "        self.config = config\n",
    "        self.W_dq  = nn.Linear(config.n_embd, config.q_latent_dim , False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # (NoPE)\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "        self.W_uq  = nn.Linear(config.q_latent_dim , config.n_embd, False)\n",
    "        self.W_dkv = nn.Linear(config.n_embd, config.kv_latent_dim, False)\n",
    "        self.W_uk  = nn.Linear(config.kv_latent_dim, config.n_embd, False)\n",
    "        self.W_uv  = nn.Linear(config.kv_latent_dim, config.n_embd, False)\n",
    "\n",
    "        # (RoPE)\n",
    "        self.W_qr  = nn.Linear(config.q_latent_dim, config.n_head * config.rope_head_dim,  False)\n",
    "        self.W_kr  = nn.Linear(config.n_embd, config.rope_head_dim, False)\n",
    "\n",
    "        # (Out)\n",
    "        self.W_o = nn.Linear(config.n_embd, config.n_embd ,False)\n",
    "\n",
    "        # Absroption during inference\n",
    "        self.register_buffer('_k_absorbed_inference', None, persistent=True)\n",
    "        self.register_buffer('_v_absorbed_inference', None, persistent=True)\n",
    "\n",
    "    def _precompute_absorbed_matrices(self):\n",
    "        \"\"\"Precomputes k_absorbed and v_absorbed for efficient inference.\"\"\"\n",
    "        # Just to be safe\n",
    "        if (self._k_absorbed_inference is not None) and (self._v_absorbed_inference is not None):\n",
    "            return \n",
    "        \n",
    "        nh, nlkv, hs, nlq = self.config.n_head, self.config.kv_latent_dim, self.config.n_embd//self.config.n_head, self.config.q_latent_dim\n",
    "        with torch.no_grad():\n",
    "            self._k_absorbed_inference = (self.W_uq.weight.view(1,nlq,nh,hs).transpose(1,2) @ self.W_uk.weight.view(1,nh,hs,nlkv))\n",
    "            self._v_absorbed_inference = (self.W_uv.weight.T @ self.W_o.weight.T).view(nlkv, nh, hs).transpose(0,1).unsqueeze(0)    \n",
    "\n",
    "    def forward(self, x:torch.Tensor, freqs_cis:torch.Tensor|None, kv_cache=None, VAL_RUN=False):\n",
    "        B,T,C = x.size()\n",
    "        nh,nlkv,nlq = self.config.n_head, self.config.kv_latent_dim, self.config.q_latent_dim\n",
    "        hs = C//nh\n",
    "        dhr = self.config.rope_head_dim\n",
    "        \n",
    "        c_q:torch.Tensor = self.W_dq(x)  # (B,T,nlq)\n",
    "\n",
    " #------------ NoPE--------------\n",
    "\n",
    "        # Define the absorbed matrices\n",
    "        if self.training or VAL_RUN:  # HIDDEN IN PLAIN SIGHT : THIS BUG TOOK ~16 HRS TO DEBUG\n",
    "            k_eff = (self.W_uq.weight.view(1,nlq,nh,hs).transpose(1,2) @ self.W_uk.weight.view(1,nh,hs,nlkv))\n",
    "            v_eff = (self.W_uv.weight.T @ self.W_o.weight.T).view(nlkv, nh, hs).transpose(0,1).unsqueeze(0)  \n",
    "        else:\n",
    "            if (self._k_absorbed_inference is None) or (self._v_absorbed_inference is None):\n",
    "                self._precompute_absorbed_matrices()\n",
    "            k_eff = self._k_absorbed_inference\n",
    "            v_eff = self._v_absorbed_inference\n",
    "\n",
    "        new_c_kv = self.W_dkv(x) # down projection : (B,T,C) -> (B,T,n_kvl)\n",
    "\n",
    "        if kv_cache is None: # first pass\n",
    "            c_kv = new_c_kv # (B,T,n_kvl) ; initiate cache\n",
    "        else:\n",
    "            c_kv = torch.cat([kv_cache['c_kv'], new_c_kv], dim=1) # append cache\n",
    "\n",
    "        T_full = c_kv.size(1) # Current total sequence length (including cache)\n",
    "\n",
    "        attn_c = c_q.unsqueeze(1) @ k_eff @ c_kv.transpose(-1,-2).unsqueeze(1)\n",
    "\n",
    " #------------ RoPE--------------\n",
    "\n",
    "        c_kr:torch.Tensor = self.W_kr(x).unsqueeze(2)        # (B,T,1,dhr)\n",
    "        k_r = apply_rotary_emb(c_kr, freqs_cis).transpose(1,2)  # (B,1,T,dhr), to be cached\n",
    "\n",
    "        # initate KV cache\n",
    "        if kv_cache is not None:\n",
    "            k_r = torch.cat([kv_cache['k_r'], k_r], dim=2)\n",
    "\n",
    "        c_qr:torch.Tensor = self.W_qr(c_q).view(B,T,nh,dhr) # (B,T,nh,dhr) # because rope expects (B,T,H,dh)\n",
    "        q_r = apply_rotary_emb(c_qr, freqs_cis).transpose(1,2) # (B,nh,T,dhr)\n",
    "        \n",
    "        attn_r = q_r @ k_r.transpose(-1,-2)\n",
    "\n",
    " #------------ Out--------------\n",
    "\n",
    "        attn = (attn_c + attn_r)/math.sqrt(hs+dhr)\n",
    "\n",
    "        # query_indices = torch.arange(T, device=x.device).unsqueeze(1) + (T_full - T)\n",
    "        # key_indices = torch.arange(T_full, device=x.device).unsqueeze(0)\n",
    "        # mask = (query_indices >= key_indices).unsqueeze(0).unsqueeze(0) # (1,1,T,T_full)\n",
    "        # attn = attn.masked_fill(mask == 0, float('-inf')) \n",
    "\n",
    "        mask = torch.triu(torch.ones(T, T_full, device=x.device, dtype=torch.bool), diagonal=T_full - T + 1)\n",
    "        attn = attn.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn:torch.Tensor = self.dropout(F.softmax(attn, dim=-1)) # (B, nh, T, T_full)\n",
    "\n",
    "        # final output : attn @ C_kv @ v_abs \n",
    "        # (B, nh, T, T) * (B, 1, T, n_kvl) * (1, nh, n_kvl, hs) = (B, nh, T, hs)\n",
    "        y:torch.Tensor = attn @ c_kv.unsqueeze(1) @ v_eff #(B, nh, T, hs)\n",
    "        y = self.dropout(y.transpose(1,2).contiguous().view(B,T,C))\n",
    "\n",
    "        updated_kv_cache = {'c_kv': c_kv, 'k_r': k_r}\n",
    "\n",
    "        return y, updated_kv_cache\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Routes the attention mechanism according to the config\"\"\"\n",
    "\n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if config.attn in ('mha','mqa','gqa'):\n",
    "            self.attn = GQA(config)\n",
    "        \n",
    "        elif config.attn == 'mla':\n",
    "            if config.pos_emb != 'rope':\n",
    "                self.attn = NaiveMHLA(config)\n",
    "            else:\n",
    "                self.attn = FullMHLA(config)\n",
    "                \n",
    "    def forward(self, x:torch.Tensor, freqs_cis:torch.Tensor|None = None, kv_cache=None, VAL_RUN=False):\n",
    "        return self.attn(x, freqs_cis, kv_cache, VAL_RUN)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" A simple feed-forward network block. \"\"\"\n",
    "    def __init__(self, config: LLMconfig):\n",
    "        super().__init__()\n",
    "        self.non_linearity = config.non_linearity.lower()\n",
    "        \n",
    "        if self.non_linearity == 'swiglu':\n",
    "            # One projection, then split into two halves\n",
    "            self.c_fc = nn.Linear(config.n_embd, 2 * config.up_dim, bias=False)\n",
    "            self.c_proj = nn.Linear(config.up_dim, config.n_embd, bias=False)\n",
    "        else:\n",
    "            non_linearity_map = {\n",
    "                'relu': nn.ReLU(), 'gelu': nn.GELU(), 'swish': nn.SiLU(), 'mish': nn.Mish(),\n",
    "                'silu': nn.SiLU(), 'selu': nn.SELU(), 'celu': nn.CELU(), 'elu': nn.ELU(),\n",
    "                'glu' : nn.GLU(), 'sigmoid': nn.Sigmoid(),\n",
    "                'lrelu': nn.LeakyReLU(negative_slope=0.01), 'tanh': nn.Tanh()\n",
    "            }\n",
    "            self.c_fc = nn.Linear(config.n_embd, config.up_dim, bias=False)\n",
    "            self.non_linearity_func = non_linearity_map.get(self.non_linearity, nn.GELU())\n",
    "            self.c_proj = nn.Linear(config.up_dim, config.n_embd, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.non_linearity == 'swiglu':\n",
    "            x1, x2 = self.c_fc(x).chunk(2, dim=-1)\n",
    "            x = F.silu(x1) * x2\n",
    "        else:\n",
    "            x = self.c_fc(x)\n",
    "            x = self.non_linearity_func(x)\n",
    "        \n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" A single feed-forward network expert. \"\"\"\n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        self.expert = MLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.expert(x)\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    '''\n",
    "    This class implements the DeepSeekMoE layer, featuring shared and routed experts.\n",
    "    It uses an Auxiliary-Loss-Free load balancing strategy with a dynamic bias term.\n",
    "    Ref: https://arxiv.org/pdf/2412.19437\n",
    "    '''\n",
    "\n",
    "    def __init__(self, config: LLMconfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # first `n_shared` are shared, the rest are routed\n",
    "        self.n_shared = config.n_shared\n",
    "        self.n_routed = config.n_exp - config.n_shared\n",
    "        \n",
    "        # Number of experts to activate from the ROUTED pool\n",
    "        self.n_act_routed = config.n_act - config.n_shared\n",
    "        assert self.n_act_routed > 0, \"Number of active experts must be greater than shared experts\"\n",
    "\n",
    "        self.experts = nn.ModuleList([Expert(config) for _ in range(config.n_exp)])\n",
    "        self.gate = nn.Linear(config.n_embd, self.n_routed, bias=False)\n",
    "        \n",
    "        if config.aux_free:\n",
    "            self.register_buffer('expert_bias', torch.zeros(self.n_routed))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Forward pass for the DeepSeekMoE layer with Aux-Loss-Free Balancing. \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C)  # Shape: (B*T, C)\n",
    "        n_tokens = x_flat.shape[0]\n",
    "\n",
    "        # ___________ SHARED EXPERT PATH ___________\n",
    "\n",
    "        shared_output = torch.zeros_like(x_flat)\n",
    "        if self.n_shared > 0:\n",
    "            for i in range(self.n_shared):\n",
    "                shared_output += self.experts[i](x_flat) # bypass the router\n",
    "\n",
    "        #  ___________ ROUTED EXPERT PATH ___________\n",
    "\n",
    "        router_logits = self.gate(x_flat)\n",
    "\n",
    "        if self.config.aux_free:        \n",
    "            # Add Bias and then select topk\n",
    "            biased_router_logits = router_logits + self.expert_bias\n",
    "            topk_biased_logits, topk_indices = torch.topk(biased_router_logits, self.n_act_routed, dim=1)\n",
    "\n",
    "            # Gating weights are based on un-biased logits\n",
    "            topk_original_logits = torch.gather(router_logits, 1, topk_indices) \n",
    "            topk_gates = F.softmax(topk_original_logits, dim=1)\n",
    "\n",
    "            # Calculate expert load and update bias during training only\n",
    "            with torch.no_grad():\n",
    "                ones = torch.ones_like(topk_indices, dtype=x_flat.dtype)\n",
    "                fi_counts = torch.zeros(self.n_routed, device=x.device).scatter_add_(0, topk_indices.flatten(), ones.flatten())\n",
    "                fi = fi_counts / n_tokens\n",
    "\n",
    "            if self.training:\n",
    "                with torch.no_grad():\n",
    "                    ideal_load = 1.0 / self.n_routed\n",
    "                    delta = ideal_load - fi \n",
    "                    self.expert_bias += (self.config.gamma*delta)\n",
    "\n",
    "            router_probs = F.softmax(router_logits, dim=1)\n",
    "            pi = router_probs.mean(dim=0)\n",
    "            aux_loss = self.config.alpha * self.n_routed * torch.sum(pi*fi)\n",
    "\n",
    "        else:\n",
    "            router_probs = F.softmax(router_logits, dim=1)\n",
    "            pi = router_probs.mean(dim=0)\n",
    "            \n",
    "            topk_logits, topk_indices = torch.topk(router_logits, self.n_act_routed, dim=1)\n",
    "            ones = torch.ones_like(topk_indices, dtype=torch.float)\n",
    "            fi_counts = torch.zeros(self.n_routed, device=x.device).scatter_add_(0, topk_indices.flatten(), ones.flatten())\n",
    "            fi = fi_counts / n_tokens\n",
    "\n",
    "            aux_loss = self.config.coeff * self.n_routed * torch.sum(pi * fi)\n",
    "\n",
    "            topk_gates = F.softmax(topk_logits, dim=1)  \n",
    "\n",
    "        # Dispatch\n",
    "        routed_output = torch.zeros_like(x_flat)\n",
    "\n",
    "        for i in range(self.n_routed):\n",
    "            token_indices, topk_slot = (topk_indices == i).nonzero(as_tuple=True)\n",
    "            if token_indices.numel() > 0:\n",
    "                tokens_for_expert = x_flat[token_indices]\n",
    "                gates_for_expert = topk_gates[token_indices, topk_slot].unsqueeze(1)\n",
    "\n",
    "                # access the expert using an offset of `n_shared`\n",
    "                expert_output = self.experts[i + self.n_shared](tokens_for_expert)\n",
    "                \n",
    "                weighted_output = expert_output * gates_for_expert\n",
    "                routed_output.index_add_(0, token_indices, weighted_output)\n",
    "        \n",
    "        # combine to output\n",
    "        y = (shared_output + routed_output).view(B, T, C)\n",
    "        return y, aux_loss\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" A single Transformer block combining attention and MLP. \"\"\"\n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        self.is_moe = config.moe\n",
    "        self.attn = Attention(config)\n",
    "        self.ln1  = LayerNorm(config, config.n_embd)\n",
    "        self.ln2  = LayerNorm(config, config.n_embd)\n",
    "        if config.moe:\n",
    "            self.moe = MoE(config)\n",
    "        else:\n",
    "            self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, freqs_cis:torch.Tensor|None = None, kv_cache=None, VAL_RUN=False):\n",
    "        # Layer Norm + Attention\n",
    "        attn_output, updated_kv_cache = self.attn.forward(self.ln1(x), freqs_cis, kv_cache, VAL_RUN)\n",
    "        x = x + attn_output\n",
    "\n",
    "        if self.is_moe: \n",
    "            moe_output, aux_loss = self.moe(self.ln2(x))\n",
    "            x = x + moe_output\n",
    "        else:\n",
    "            aux_loss = 0.0\n",
    "            x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x, updated_kv_cache, aux_loss\n",
    "\n",
    "class _transformer_container(nn.Module):\n",
    "    '''For type checking in LLM class'''\n",
    "    drop:nn.Dropout\n",
    "    h:list[Block]\n",
    "    ln_f:LayerNorm\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    \"\"\" A simple Large language model \"\"\"\n",
    "    def __init__(self, config:LLMconfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tkn_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        if config.pos_emb == 'learn':\n",
    "            self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n",
    "        elif config.pos_emb == 'sin':\n",
    "            pos_emb  = torch.zeros(config.block_size, config.n_embd)\n",
    "            position = torch.arange(0, config.block_size, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, config.n_embd, 2).float() * (-math.log(10000.0) / config.n_embd))\n",
    "            pos_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "            pos_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "            self.register_buffer('pos_emb', pos_emb)\n",
    "        elif config.pos_emb == 'rope':\n",
    "            fcl = self._precompute_freqs_cis_layers() \n",
    "            for i,fci in enumerate(fcl): self.register_buffer(f\"freqs_cis_{i}\", fci)\n",
    "            del fcl\n",
    "            # self.register_buffer(\"freqs_cis_layers\", self._precompute_freqs_cis_layers(), persistent=False)\n",
    "    \n",
    "        self.transformer:_transformer_container = nn.ModuleDict(dict(\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h    = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config, config.n_embd)))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.tkn_emb.weight = self.lm_head.weight # weight tying\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        self.VAL_RUN=False\n",
    "        if config.act_recomp: print(\"Using Activation Recomputation\")\n",
    "\n",
    "    def _precompute_freqs_cis_layers(self):\n",
    "        \"\"\"Precomputes the rotary frequencies for each layer for RoPE.\"\"\"\n",
    "        freqs_cis_layers = []\n",
    "        for layer_config in self.config.layer_configs:\n",
    "            d = layer_config.rope_head_dim if layer_config.attn=='mla' else layer_config.n_embd//layer_config.n_head\n",
    "            assert d % 2 == 0, \"head dimension must be even\"\n",
    "            theta = 1.0 / (10000.0 ** (torch.arange(0, d, 2).float() / d)) # 1.0 / (base^(2i/d))\n",
    "            seq = torch.arange(self.config.block_size)\n",
    "            freqs = torch.outer(seq, theta)\n",
    "            # Convert to complex numbers: r * e^(i*theta)\n",
    "            freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "            if layer_config.pos_emb=='rope': freqs_cis_layers.append(freqs_cis) # Should always happen \n",
    "            else: freqs_cis_layers.append(None) # Should never happen\n",
    "\n",
    "        # return torch.stack(freqs_cis_layers, dim=0) # register_buffer only accepts a torch.tensor\n",
    "        return freqs_cis_layers\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initializes model weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def get_num_params(self):\n",
    "        \"\"\"Returns the total number of parameters and active parameters in the model.\"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        \n",
    "        active_params = 0\n",
    "\n",
    "        active_params += self.tkn_emb.weight.numel()      # embeddings\n",
    "        if self.config.pos_emb == 'learn': active_params += self.pos_emb.weight.numel()\n",
    "        if self.config.norm == 'layer': active_params += self.transformer.ln_f.bias.numel()\n",
    "        active_params += self.transformer.ln_f.weight.numel() \n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            active_params += sum(p.numel() for p in block.attn.parameters())   # ----|\n",
    "            active_params += sum(p.numel() for p in block.ln1.parameters())    #     |---> Always active\n",
    "            active_params += sum(p.numel() for p in block.ln2.parameters())    # ----|\n",
    "\n",
    "            if block.is_moe:\n",
    "\n",
    "                active_params += sum(p.numel() for p in block.moe.gate.parameters())                # ----|\n",
    "                for i in range(block.moe.n_shared):                                                 #     |---> Always active\n",
    "                    active_params += sum(p.numel() for p in block.moe.experts[i].parameters())      # ----|\n",
    "\n",
    "                if block.moe.n_routed > 0:\n",
    "                    # Calculate params for one routed expert, multiply by the number of active ones\n",
    "                    params_per_routed_expert = sum(p.numel() for p in block.moe.experts[block.moe.n_shared].parameters())\n",
    "                    active_params += block.moe.n_act_routed * params_per_routed_expert\n",
    "            \n",
    "            else: # In case a block is not MoE\n",
    "                active_params += sum(p.numel() for p in block.mlp.parameters())\n",
    "\n",
    "        return n_params, active_params\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for p in param_dict.values() if p.dim() >= 2]\n",
    "        nodecay_params = [p for p in param_dict.values() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}]\n",
    "\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        try:\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, fused=True)\n",
    "            print(\"Using Fused AdamW\")\n",
    "        except:\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        \"\"\"\n",
    "        Prepares inputs for generation. This is a standard method expected by\n",
    "        Hugging Face's generation utilities.\n",
    "        \"\"\"\n",
    "        # The model_inputs dictionary will be passed to the forward method.\n",
    "        # It needs to contain all the arguments that the forward method accepts.\n",
    "        model_inputs = {\"idx\": input_ids}\n",
    "\n",
    "        # The `kwargs` dictionary might contain other useful parameters like `past_key_values`.\n",
    "        # In your case, you are using `kv_caches`.\n",
    "        model_inputs.update(kwargs)\n",
    "        return model_inputs\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets:torch.Tensor|None=None, kv_caches:list[torch.Tensor]|None=None):\n",
    "        B, T = idx.size()\n",
    "        start_pos = 0\n",
    "\n",
    "        if kv_caches is not None and kv_caches[0] is not None:\n",
    "            first_layer_config = self.config.layer_configs[0]\n",
    "            if first_layer_config.attn in ('mha', 'mqa', 'gqa'):\n",
    "                start_pos = kv_caches[0][0].shape[-2]\n",
    "            elif first_layer_config.attn == 'mla':\n",
    "                if first_layer_config.pos_emb == 'rope':\n",
    "                    start_pos = kv_caches[0]['c_kv'].shape[1]\n",
    "                else: # Naive MLA (tensor cache)\n",
    "                    start_pos = kv_caches[0].shape[1]\n",
    "\n",
    "        tkn_emb = self.tkn_emb(idx)  # Shape: (B, T, n_embd)\n",
    "        \n",
    "        x = tkn_emb # Default value for x\n",
    "        freqs_cis_layers = [None for _ in range(self.config.n_layer)]\n",
    "        if self.config.pos_emb == 'rope':\n",
    "            freqs_cis_layers = [getattr(self, f\"freqs_cis_{i}\")[start_pos : start_pos + T] for i in range(self.config.n_layer)]\n",
    "\n",
    "        elif self.config.pos_emb == 'learn':\n",
    "            pos = torch.arange(start_pos, start_pos + T, dtype=torch.long, device=idx.device)\n",
    "            x = tkn_emb + self.pos_emb(pos)\n",
    "\n",
    "        elif self.config.pos_emb == 'sin':\n",
    "            pos = torch.arange(start_pos, start_pos + T, dtype=torch.long, device=idx.device)\n",
    "            x = tkn_emb + self.pos_emb[pos]\n",
    "\n",
    "        x = self.transformer.drop(x)\n",
    "\n",
    "        if kv_caches is None:\n",
    "            kv_caches = [None] * self.config.n_layer\n",
    "        \n",
    "        updated_kv_caches = []\n",
    "        total_aux_loss = 0.0\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "            # The block now returns an auxiliary loss from the MoE layer\n",
    "            if not self.config.act_recomp: \n",
    "                x, updated_kv_cache, aux_loss = block(x, freqs_cis_layers[i], kv_caches[i], self.VAL_RUN)\n",
    "            else : \n",
    "                x, updated_kv_cache, aux_loss = checkpoint(block, x, freqs_cis_layers[i], kv_caches[i], self.VAL_RUN)\n",
    "\n",
    "            updated_kv_caches.append(updated_kv_cache)\n",
    "            total_aux_loss += aux_loss\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits:torch.Tensor = self.lm_head(x)\n",
    "            main_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            # Add the accumulated auxiliary loss to the main loss\n",
    "            # We divide by the number of layers because loss is accumulated from each MoE block\n",
    "            loss = main_loss + total_aux_loss / self.config.n_layer\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss, updated_kv_caches\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0, topk: int | None = None, EOT:int=None):\n",
    "        self.eval()\n",
    "        kv_caches = [None] * self.config.n_layer\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop context to block size, rolling context window\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # for first token, use full prompt, and only last token for subsequent tokens\n",
    "            input_for_forward = idx_cond if kv_caches[0] is None else idx[:, -1:]\n",
    "\n",
    "            # handle the rolling KV cache\n",
    "            if kv_caches[0] is not None:\n",
    "                # determine current cache length from any layer, say first\n",
    "                first_layer_config = self.config.layer_configs[0]\n",
    "                if first_layer_config.attn in ('mha', 'mqa', 'gqa'):\n",
    "                    cache_len = kv_caches[0][0].shape[-2]\n",
    "                elif first_layer_config.attn == 'mla':\n",
    "                     cache_len = kv_caches[0]['c_kv'].shape[1] if self.config.pos_emb == 'rope' else kv_caches[0].shape[1]\n",
    "                else: cache_len = 0 # should never happen\n",
    "\n",
    "                # actual handling part\n",
    "                if cache_len >= self.config.block_size:\n",
    "                    # Keep the most recent (block_size - 1) tokens to make space for the new one\n",
    "                    keep_len = self.config.block_size - 1\n",
    "                    for layer_idx, layer_config in enumerate(self.config.layer_configs):\n",
    "                        layer_cache = kv_caches[layer_idx]\n",
    "                        if layer_config.attn in ('mha', 'mqa', 'gqa'):\n",
    "                            k, v = layer_cache\n",
    "                            kv_caches[layer_idx] = (k[..., -keep_len:, :], v[..., -keep_len:, :])\n",
    "                        elif layer_config.attn == 'mla':\n",
    "                            if self.config.pos_emb == 'rope':\n",
    "                                layer_cache['c_kv'] = layer_cache['c_kv'][:, -keep_len:, :]\n",
    "                                layer_cache['k_r']  = layer_cache['k_r'][:, :, -keep_len:, :] # Seq len is dim 2\n",
    "                            else: # c_kv\n",
    "                                kv_caches[layer_idx] = layer_cache[:, -keep_len:, :]\n",
    "\n",
    "            # The forward pass now returns three items; we only need logits and caches for generation\n",
    "            logits, _, kv_caches = self(input_for_forward, kv_caches=kv_caches)\n",
    "            logits = logits[:, -1, :] # logits for final token\n",
    "\n",
    "            if temperature == 0.0:\n",
    "                # greedy sampling, deterministic, \"hardmax\" instead of softmax\n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True) \n",
    "            else:\n",
    "                logits = logits / temperature # this is actuallt the randomness or 'creativty' part\n",
    "                if topk is not None:\n",
    "                    v, _ = torch.topk(logits, min(topk, logits.size(-1))) # pick topk values\n",
    "                    logits[logits < v[:, [-1]]] = -float('inf') # mask out other logits\n",
    "                probs = F.softmax(logits, dim=-1) # generate a probability distribution\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) # sample from probabilty distribution\n",
    "            \n",
    "            if EOT is not None and (idx_next.item() == EOT): break\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        self.train()\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1576865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def apply_rotary_emb(x:torch.Tensor, freqs_cis:torch.Tensor)->torch.Tensor:\n",
    "    ''' Applies RoPE to either the query or the key whose embeddings are to be rotated two at a time.'''\n",
    "\n",
    "    # H below is either the number of total query heads(nh)\n",
    "    # hs is the embedding dimension for the query/key, given by n_embd//nh\n",
    "    B,T,H,_ = x.size()\n",
    "    x_ = x.float().reshape(B, T, H, -1, 2)          # (B, T, H, hs)       -> (B, T, H, hs//2, 2)    -> creates the two pairs in the embd dim\n",
    "    x_re, x_im = x_.unbind(-1)                      # (B, T, H, hs//2, 2) -> (B, T, H, hs//2)       -> splits those two pairs\n",
    "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2) # (T, hs//2)          -> (1, T, 1, hs//2)       -> this has dtype complex64, so last dim has two parts, real and imaginary\n",
    "    # freqs_cis has two parts : real and imaginary (cosθ, sinθ)\n",
    "    # import code ; code.interact(local=locals())\n",
    "    # Perform the rotation (vector * rotation matrix)\n",
    "    x_re_out = x_re*freqs_cis.real - x_im*freqs_cis.imag    # (B, T, H, hs//2) * (1, T, 1, hs//2) - (B, T, H, hs//2) * (1, T, 1, hs//2) -> (B, T, H, hs//2)\n",
    "    x_im_out = x_re*freqs_cis.imag + x_im*freqs_cis.real    # (B, T, H, hs//2) * (1, T, 1, hs//2) + (B, T, H, hs//2) * (1, T, 1, hs//2) -> (B, T, H, hs//2)\n",
    "    \n",
    "    # Stack the real and imaginary parts back together\n",
    "    x_out = torch.stack([x_re_out, x_im_out], dim=-1).flatten(3) # (B, T, H, hs//2), (B, T, H, hs//2) -> (B, T, H, hs)\n",
    "\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "def get_lr(iter, TrainingConfig:Trainconfig):\n",
    "    max_lr = TrainingConfig.learning_rate\n",
    "    min_lr = max_lr*0.1\n",
    "    max_decay_steps = TrainingConfig.max_iters + 2 # avoid division by zero\n",
    "    # 1) linear warump for warmup_steps:\n",
    "    if iter < TrainingConfig.warmup_steps:\n",
    "        return max_lr * (iter+1)/TrainingConfig.warmup_steps\n",
    "    #2) if iter > lr_decay_iters, return min_lr\n",
    "    elif iter > max_decay_steps:\n",
    "        return min_lr\n",
    "    #3) in between, use cosine decay\n",
    "    else:\n",
    "        decay_ratio = (iter - TrainingConfig.warmup_steps) / (max_decay_steps - TrainingConfig.warmup_steps)\n",
    "        decay_ratio = min(decay_ratio, 1.0)  # ensure it does\n",
    "        coeff = 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
    "        return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model:LLM, TrainingConfig:Trainconfig, train_loader:DataLoader, val_loader:DataLoader):\n",
    "    out = {}\n",
    "    model.eval() ; model.VAL_RUN = True\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        losses = torch.zeros(TrainingConfig.eval_iters)\n",
    "        for k in range(TrainingConfig.eval_iters):\n",
    "            X, Y = loader.next_batch()\n",
    "            with ctx:\n",
    "                _, loss, _ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train(); model.VAL_RUN = False\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "with open('mfu_config_1.yaml','r') as f: model_kwargs = yaml.load(f)\n",
    "model_config = LLMconfig(**model_kwargs)\n",
    "model = LLM(model_config).to('cuda')\n",
    "total,_ = model.get_num_params()\n",
    "print(f\"total params : {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Trainer\n",
    "with open('simple_train_config.yaml','r') as f: train_kwargs = yaml.load(f)\n",
    "train_config = Trainconfig(**train_kwargs)\n",
    "# optimzer = model.configure_optimizers(weight_decay=0.1, learning_rate=train_config.learning_rate, device='cuda')\n",
    "\n",
    "# Load data: tinystories\n",
    "train_loader = DataLoader(B=train_config.batch_size, \n",
    "                          T=model_config.block_size, \n",
    "                          file_path=f'../data/{train_config.dataset}/train.bin',\n",
    "                          device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9426ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "x,y = train_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f179fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
